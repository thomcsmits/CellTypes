{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell type composition workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import anndata as ad\n",
    "import zarr\n",
    "import scanpy as sc\n",
    "\n",
    "import requests\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import networkx as nx\n",
    "import obonet\n",
    "\n",
    "import altair as alt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve the anndata either through the remote zarr storage, or by downloading the 'secondary_analysis.h5ad' locally. Both methods are shown below.\n",
    "\n",
    "Regardless of method, we reshape the dataframe with cell counts in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: automatically detect what the current column name is from the currently used options in HuBMAP\n",
    "#TODO: remove addictional predicted_id if not existing\n",
    "\n",
    "def get_counts_from_adata_single(adata, index = 0, colname_predicted_id = 'predicted_CLID', colname_predicted_label = 'predicted_label'):\n",
    "    ''''\n",
    "    Calculate cell counts from anndata object. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    adata : anndata object\n",
    "    index : str or int\n",
    "        name for sample in dataframe\n",
    "    colname_predicted_id : str\n",
    "        column name in adata.obs with cell ID. If not present, set this to None. Default: 'predicted_CLID'\n",
    "    colname_predicted_label : str\n",
    "        column name in adata.obs with cell label. Default: 'predicted_label'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataframe with cell count and cell fraction for each cell type in sample\n",
    "    '''\n",
    "    if(colname_predicted_id == None): \n",
    "        df_adata = adata.obs[[colname_predicted_label]].value_counts().to_frame().reset_index().rename(columns={colname_predicted_label:'cell_type_label', 0:'cell_count'})\n",
    "        df_adata['cell_type_CLID'] = df_adata['cell_type_label']\n",
    "    else: \n",
    "        df_adata = adata.obs[[colname_predicted_id, colname_predicted_label]].value_counts().to_frame().reset_index().rename(columns={colname_predicted_id:'cell_type_CLID', colname_predicted_label:'cell_type_label', 0:'cell_count'})\n",
    "    df_adata['sample_id'] = index\n",
    "    df_adata['sample_n_cat'] = df_adata.shape[0]\n",
    "    df_adata['sample_count'] = df_adata['cell_count'].sum()\n",
    "    df_adata['cell_fraction'] = df_adata['cell_count'] / df_adata['sample_count']\n",
    "    df_adata[['sample_id', 'cell_type_CLID', 'cell_type_label', 'cell_count', 'cell_fraction', 'sample_count', 'sample_n_cat']]\n",
    "    return df_adata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remote zarr storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: allow remote retrieval through hubmap-ids instead of uuids\n",
    "#TODO: allow verification to retrieve datasets with protected access\n",
    "\n",
    "def load_remote_anndata(uuids):\n",
    "    '''\n",
    "    For a list of uuids, retrieve anndata from remote zarr storage, and wrangle to dataframe of cell counts with standard structure\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    uuids : list of strings\n",
    "        list of uuids for which to retrieve data\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    adata_dict : dict \n",
    "    df_adata_dict : dict\n",
    "    df_adata_comb : pandas dataframe\n",
    "    '''\n",
    "\n",
    "    def load_remote_anndata_zarr_single(zarr_url):\n",
    "        '''\n",
    "        For a given url, create anndata object with all columns of obs and X_pca and X_umap in remote storage.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        zarr_url : string \n",
    "            Url of zarr storage\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        anndata object\n",
    "        '''\n",
    "        root = zarr.open(zarr_url)\n",
    "        obs_index = root.obs['_index']\n",
    "        obs_df = pd.DataFrame(index = obs_index)\n",
    "        obs_attrs = dict(root.obs.attrs)['column-order']\n",
    "\n",
    "        for att in obs_attrs: \n",
    "            obs_df[att] = root.obs[att]\n",
    "\n",
    "        obsm = {}\n",
    "        \n",
    "        for mapping in ['X_pca', 'X_umap']:\n",
    "            obsm[mapping] = root.obsm[mapping]\n",
    "        \n",
    "        adata = ad.AnnData(obs=obs_df, obsm=obsm)\n",
    "        return adata\n",
    "    \n",
    "\n",
    "    zarr_base = 'https://assets.hubmapconsortium.org/'\n",
    "    zarr_ext = '/hubmap_ui/anndata-zarr/secondary_analysis.zarr'\n",
    "\n",
    "    adata_dict = {}\n",
    "    df_adata_dict = {}\n",
    "\n",
    "    for index, id in enumerate(uuids): \n",
    "        zarr_url = zarr_base + id + zarr_ext\n",
    "        adata_i = load_remote_anndata_zarr_single(zarr_url)\n",
    "        adata_dict[id] = adata_i\n",
    "        df_adata_i = get_counts_from_adata_single(adata_i, id, colname_predicted_id = None, colname_predicted_label = 'predicted.ASCT.celltype')\n",
    "        df_adata_dict[id] = df_adata_i\n",
    "    \n",
    "    df_adata_comb = pd.concat(df_adata_dict).reset_index(drop=True)\n",
    "\n",
    "    return adata_dict, df_adata_dict, df_adata_comb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's retrieve some data that is openly accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uuids_open = ['d1e9f509063984326570cf603c5654f2', 'c1a0c043d0a986d71552091cc1b09742', 'f4b3c0e24e09e1e8c16f2e4e190a8cbb']\n",
    "adata_dict, df_adata_dict, df_adata_comb = load_remote_anndata(uuids_open)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adata_comb.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_local_anndata(data_path, file_list = None):\n",
    "    '''\n",
    "    Retrieve anndata from local storage, and wrangle to dataframe of cell counts with standard structure\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_path : str\n",
    "        absolute or relative path to folder where .h5ad files are stored\n",
    "    file_list : list of str\n",
    "        list of file names to retrieve. If None, all files in folder are read. Default: None\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    adata_dict : dict \n",
    "    df_adata_dict : dict\n",
    "    df_adata_comb : pandas dataframe\n",
    "    '''\n",
    "\n",
    "    if file_list == None: \n",
    "        file_list = os.listdir(data_path)\n",
    "    \n",
    "    adata_dict = {}\n",
    "    df_adata_dict = {}\n",
    "    for index, file in enumerate(file_list): \n",
    "        adata_i = ad.read(data_path + file)\n",
    "        adata_dict[file] = adata_i\n",
    "        df_adata_i = get_counts_from_adata_single(adata_i, file)\n",
    "        df_adata_dict[file] = df_adata_i\n",
    "\n",
    "    df_adata_comb = pd.concat(df_adata_dict).reset_index(drop=True)\n",
    "    \n",
    "    return adata_dict, df_adata_dict, df_adata_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './../data/raw/h5ad/protected-access/'\n",
    "adata_dict, df_adata_dict, df_adata_comb = load_local_anndata(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adata_comb.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also nice to be able to generate vectors of the counts for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(df_adata_comb):\n",
    "    df_embs = df_adata_comb.pivot(index = 'sample_id', columns = 'cell_type_CLID')['cell_count'].fillna(0)\n",
    "    df_embs_f = df_embs.div(df_embs.sum(axis=1), axis=0)\n",
    "\n",
    "    return df_embs, df_embs_f"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining one sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './../data/raw/h5ad/protected-access/'\n",
    "file_list = ['HBM299.VDWT.444.h5ad']\n",
    "adata_dict_HBM299, df_adata_dict_HBM299, df_HBM299 = load_local_anndata(data_path, file_list = file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(adata_dict_HBM299[file_list[0]], color = \"predicted_CLID\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display the composition of cell types in various ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = alt.Chart(df_HBM299).mark_bar().encode(\n",
    "    x = alt.X('sample_id:O', axis = alt.Axis(title = 'ID')),\n",
    "    y = alt.Y('cell_fraction:Q', axis = alt.Axis(title ='Fraction'), scale=alt.Scale(domain=[0, 1])),\n",
    "    color = alt.Color('cell_type_CLID:N', sort = 'y'),\n",
    "    order = alt.Order('cell_fraction', sort = 'descending'),\n",
    "    tooltip=['cell_type_CLID', 'cell_type_label', 'cell_fraction']\n",
    ").properties(\n",
    "    title = str(1) + ' cells; ' + str(1) + ' types of cells'\n",
    ").interactive()\n",
    "\n",
    "g1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: retrieve colours UMAP, use same colours"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are many very small fractions, we might want to collapse those together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_other = True\n",
    "threshold = 0.01\n",
    "\n",
    "def combine_other_fractions(df, threshold):\n",
    "    '''\n",
    "    Given a threshold, combine all cell types with a fraction below this threshold together into one row called 'other'. \n",
    "    Set 'cell_fraction_sort' for dataframe for sorting in Altair (to make sure 'other' is always on the bottom)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas dataframe\n",
    "        counts dataframe\n",
    "    threshold : double\n",
    "        threshold under which to collapse cell type\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas dataframe with collapsed row 'other'\n",
    "    '''\n",
    "    keep = df[df['cell_fraction'] >= threshold]\n",
    "    keep['cell_fraction_sort'] = keep['cell_fraction']\n",
    "    removed = df[df['cell_fraction'] < threshold]\n",
    "    \n",
    "    other = removed.groupby(['sample_id']).agg({'cell_type_CLID':'first', 'cell_type_label':'first', 'cell_count': 'sum', 'sample_id': 'first', 'sample_n_cat': 'first', 'sample_count': 'first', 'cell_fraction': 'sum'})\n",
    "    other['cell_type_CLID'] = 'other'\n",
    "    other['cell_type_label'] = 'other'\n",
    "    other['cell_fraction_sort'] = 0\n",
    "    other = other.reset_index(drop = True)\n",
    "    \n",
    "    comb = pd.concat([keep, other])\n",
    "\n",
    "    return comb\n",
    "\n",
    "if combine_other:\n",
    "    df_HBM299 = combine_other_fractions(df_HBM299, threshold)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With only one sample, there is no need to stack the bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars= alt.Chart(df_HBM299).mark_bar().encode(\n",
    "    x = alt.X('cell_fraction:Q', axis = alt.Axis(title ='Fraction'), scale=alt.Scale(domain=[0, 1])),\n",
    "    y = alt.Y('cell_type_CLID:N', sort = {'field': 'cell_fraction_sort', 'order': 'descending'}, axis = alt.Axis(title = 'ID')),\n",
    "    tooltip=['cell_type_CLID', 'cell_type_label', 'cell_fraction']\n",
    ")\n",
    "\n",
    "text = bars.mark_text(\n",
    "    align='left',\n",
    "    baseline='middle',\n",
    "    dx=3 \n",
    ").encode(\n",
    "    text=alt.Text('cell_fraction:Q', format = \".0000\")\n",
    ")\n",
    "\n",
    "g2 = (bars + text).properties()\n",
    "\n",
    "g2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: add support for hovering over 'other' and seeing composition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining a few samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './../data/raw/h5ad/protected-access/'\n",
    "adata_dict_few, df_adata_dict_few, df_adata_comb_few = load_local_anndata(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## stacked bar chart\n",
    "\n",
    "g1 = alt.Chart(df_adata_comb_few).mark_bar().encode(\n",
    "    x = alt.X('sample_id:O', axis = alt.Axis(title = \"ID\")),\n",
    "    y = alt.Y('cell_fraction:Q', axis = alt.Axis(title =\"Fraction\"), scale=alt.Scale(domain=[0, 1])),#, sort = {'aggregate': [{'field': 'cell_fraction', 'op':'sum'}], 'order':'descending'}),\n",
    "    color = alt.Color('cell_type_CLID:N', sort = 'y'),\\\n",
    "    order = alt.Order('cell_fraction', sort = 'descending'),\n",
    "    tooltip=['sample_id',  'cell_fraction', 'cell_type_CLID', 'cell_type_label']\n",
    "#).transform_aggregate(\n",
    " #   comb_frac = 'sum(cell_fraction)'\n",
    ").properties(\n",
    "    #width = 1000\n",
    ").interactive() \n",
    "\n",
    "g1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: transform aggregate for ordering by sum of fractions? \n",
    "#TODO: finish examples\n",
    "#TODO: add interactivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## stacked bar chart\n",
    "\n",
    "selection = alt.selection_multi(fields=['cell_type_CLID'], bind = 'legend')\n",
    "\n",
    "g1 = alt.Chart(df_adata_comb).mark_bar().encode(\n",
    "    x = alt.X('sample_id:O', axis = alt.Axis(title = \"ID\")),\n",
    "    y = alt.Y('cell_fraction:Q', axis = alt.Axis(title =\"Fraction\"), scale=alt.Scale(domain=[0, 1])),#, sort = {'aggregate': [{'field': 'cell_fraction', 'op':'sum'}], 'order':'descending'}),\n",
    "    color = alt.Color('cell_type_CLID:N', sort = 'y'),\n",
    "    opacity = alt.condition(selection, alt.value(1), alt.value(0.2)),\n",
    "    tooltip=['sample_id',  'cell_fraction', 'cell_type_CLID', 'cell_type_label']\n",
    "#).transform_aggregate(\n",
    " #   comb_frac = 'sum(cell_fraction)'\n",
    ").add_selection(\n",
    "    selection\n",
    ")\n",
    "\n",
    "g1.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining many samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With many more samples, scalability becomes an issue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: add sample clustering to altair heatmap\n",
    "\n",
    "def order_df_heatmap(df_embs_f): \n",
    "    # Clustermap for rows only\n",
    "    g = sns.clustermap(df_embs_f, col_cluster=False, cmap=\"magma\")\n",
    "\n",
    "    # Get the reodered indices\n",
    "    reordered_indices = g.dendrogram_row.reordered_ind\n",
    "\n",
    "    # Create a dictionary\n",
    "    reordering_dict = pd.Series(reordered_indices, index=df_embs_f.index.values).to_dict()\n",
    "\n",
    "    return(reordering_dict)\n",
    "\n",
    "df_embs, df_embs_f = get_embeddings(df_adata_comb)\n",
    "#reordering_dict = order_df_heatmap(df_embs_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: add altair network to altair heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = alt.Chart(df_adata_comb).mark_rect().encode(\n",
    "\n",
    "    x = alt.X('cell_type_CLID:O', axis = alt.Axis(title = 'Cell type')),\n",
    "    y = alt.Y('sample_id:O', axis = alt.Axis(title = 'Sample')),\n",
    "    color = alt.Color('cell_fraction:Q', scale=alt.Scale(domain=[0, 1])),\n",
    "    tooltip=['cell_type_CLID',  'cell_type_label', 'sample_id', 'cell_fraction']\n",
    ").properties(\n",
    ").interactive()\n",
    "\n",
    "g.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawing networkx in altair"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Furthermore, we want to leverage the cell ontology for relations between cell types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ontologyGraph:\n",
    "    '''\n",
    "    A class used to instantiate networks of the cell ontology.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    G : networkx Graph\n",
    "    depths_graph : dict\n",
    "    shortest_paths : dict\n",
    "    id_to_name : dict\n",
    "    name_to_id : dict\n",
    "    '''\n",
    "    def __init__(self, G, root = 'CL:0000000'):\n",
    "        self.G = G\n",
    "        self.depths_graph = nx.shortest_path_length(G, target=root)\n",
    "        self.shortest_paths = nx.shortest_path(G, target=root)\n",
    "\n",
    "        ## update G to have the depths of graph in case these were not yet present\n",
    "        nx.set_node_attributes(self.G, self.depths_graph, \"depth\")\n",
    "\n",
    "        self.id_to_name = {id_: data.get('name') for id_, data in self.G.nodes(data=True)}\n",
    "        self.name_to_id = {data['name']: id_ for id_, data in self.G.nodes(data=True) if 'name' in data}\n",
    "\n",
    "\n",
    "class ontologyGraphWithData: \n",
    "    '''\n",
    "    A class used to create and store various networks for cell label nodes in a given dataframe\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    Graphs : dict\n",
    "    df : pandas dataframe\n",
    "    nodes_df : list of nodes\n",
    "    nodes_plus_df : list of nodes\n",
    "    collapsed : Boolean (default False)\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    '''\n",
    "    def __init__(self, df):\n",
    "        self.Graphs = {\"full\": None,\n",
    "                       \"tree\": None,\n",
    "                       \"subs\": None,\n",
    "                       \"collapsed\": None\n",
    "                       }\n",
    "        self.df = df\n",
    "        self.nodes_df = None\n",
    "        self.nodes_plus_df = None\n",
    "        self.collapsed = False\n",
    "\n",
    "\n",
    "    def load_values(self):\n",
    "        self.Graphs['full'] = ontologyGraph(self.load_onto())\n",
    "        self.Graphs['tree'] = ontologyGraph(self.retrieve_tree())\n",
    "        res = self.retrieve_nodes_df()\n",
    "        self.nodes_df = res[0]\n",
    "        self.nodes_plus_df = res[1] \n",
    "        self.Graphs['subs'] = ontologyGraph(res[2])\n",
    "\n",
    "    \n",
    "    def load_onto(self):\n",
    "        '''\n",
    "        Retrieve full ontology from remote location, read it as a networkx graph.\n",
    "        Ensures all nodes are connected by removing single node that is not connected\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        networkx graph\n",
    "        '''\n",
    "        G = obonet.read_obo(\"http://purl.obolibrary.org/obo/cl/cl-basic.obo\")\n",
    "\n",
    "        ## remove unconnected node\n",
    "        smallest_cc = min(nx.weakly_connected_components(G), key=len)\n",
    "        node_removed = list(smallest_cc)[0]\n",
    "        G.remove_node(node_removed)\n",
    "\n",
    "        return G\n",
    "    \n",
    "\n",
    "    def retrieve_tree(self, root=\"CL:0000000\"):\n",
    "        '''\n",
    "        Subset edges of graph such that graph is tree, by removing all edges that are not an identity relation, and keeping the shortest paths. \n",
    "\n",
    "        Returns \n",
    "        -------\n",
    "        networkx tree\n",
    "        '''\n",
    "        ## remove the edges that aren't \"is_a\"\n",
    "        e_list = [e for e in self.Graphs['full'].G.edges(keys=True) if e[2] == \"is_a\"]\n",
    "\n",
    "        G_t = self.Graphs['full'].G.edge_subgraph(e_list).copy()\n",
    "        G_t_paths = nx.shortest_path(G_t, target=root)\n",
    "\n",
    "        edges_tree_set = set()\n",
    "        for node, path in G_t_paths.items(): \n",
    "            for i in range(len(path)-1):\n",
    "                edge = (path[i], path[i+1], \"is_a\")\n",
    "                edges_tree_set.add(edge)\n",
    "        G_tree = G_t.edge_subgraph(edges_tree_set).copy()\n",
    "\n",
    "        return G_tree\n",
    "    \n",
    "\n",
    "    def retrieve_nodes_df(self, current_ontology_column=\"cell_type_CLID\"):\n",
    "        '''\n",
    "        Find relevant nodes in ontology given a counts dataframe\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        current_ontology_column : str\n",
    "            column with name of ontology ID\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        nodes_df : list of strings\n",
    "            cell ontology id's that are in the dataframe\n",
    "        nodes_plus_df : list of strings\n",
    "            cell ontology id's that are in the dataframe together with connecting nodes\n",
    "        G_sub : networkx graph\n",
    "            graph subsetted to relevant nodes\n",
    "        '''\n",
    "        nodes_df = self.df[current_ontology_column].unique()\n",
    "        nodes_plus_df = set(nodes_df)\n",
    "        nodes_plus_df.add(\"CL:0000000\")\n",
    "       \n",
    "        all_in_network = True\n",
    "        if all_in_network: \n",
    "            for node in nodes_df: \n",
    "                path_to_root_i = self.Graphs['tree'].shortest_paths[node]\n",
    "                for node in path_to_root_i: \n",
    "                    nodes_plus_df.add(node)\n",
    "\n",
    "        #fix that this is tree\n",
    "        else:   \n",
    "            iter_tup_nodes_df = itertools.combinations(nodes_df, 2)\n",
    "            iter_lca_nodes_df = nx.tree_all_pairs_lowest_common_ancestor(self.Graphs['tree'].G.reverse(), pairs = iter_tup_nodes_df)\n",
    "            for nodes, lca in iter_lca_nodes_df: \n",
    "                nodes_plus_df.add(lca)\n",
    "            \n",
    "        G_sub = self.Graphs['tree'].G.subgraph(nodes_plus_df).copy()\n",
    "\n",
    "        return(nodes_df, nodes_plus_df, G_sub)\n",
    "    \n",
    "\n",
    "    def draw_tree(self, which = 'subs'): \n",
    "        '''\n",
    "        Method for drawing tree\n",
    "        '''\n",
    "        ## fiend positions for dendrogram-like style\n",
    "\n",
    "        ## determine amount of nodes per level\n",
    "        ## ..\n",
    "        def hierarchy_pos(G, root='CL:0000000', width=1., vert_gap = 0.2, vert_loc = 0, xcenter = 0.5, pos = None, parent = None):\n",
    "            '''\n",
    "            Adapted from Joel's answer at https://stackoverflow.com/a/29597209/2966723.  \n",
    "            Licensed under Creative Commons Attribution-Share Alike \n",
    "            If the graph is a tree this will return the positions to plot this in a \n",
    "            hierarchical layout.\n",
    "            '''\n",
    "            \n",
    "            if not nx.is_tree(self.Graphs[which].G):\n",
    "                raise TypeError('cannot use hierarchy_pos on a graph that is not a tree')\n",
    "\n",
    "            if pos is None:\n",
    "                pos = {root:(xcenter,vert_loc)}\n",
    "            else:\n",
    "                pos[root] = (xcenter, vert_loc)\n",
    "            children = list(G.neighbors(root))\n",
    "            if not isinstance(G, nx.DiGraph) and parent is not None:\n",
    "                children.remove(parent)  \n",
    "            if len(children)!=0:\n",
    "                dx = width/len(children) \n",
    "                nextx = xcenter - width/2 - dx/2\n",
    "                for child in children:\n",
    "                    nextx += dx\n",
    "                    pos = hierarchy_pos(G,child, width = dx, vert_gap = vert_gap, \n",
    "                                        vert_loc = vert_loc-vert_gap, xcenter=nextx,\n",
    "                                        pos=pos, parent = root)\n",
    "            return pos          \n",
    "        \n",
    "        graph = self.Graphs[which].G\n",
    "\n",
    "        pos = hierarchy_pos(graph.reverse(), root='CL:0000000')   \n",
    "        nx.draw(graph, pos=pos, node_size = 40, with_labels = False, arrows = False, node_color = 'black')\n",
    "\n",
    "        labels = {}\n",
    "        for node in graph.nodes(): \n",
    "            if node in self.nodes_df: \n",
    "                labels[node] = node\n",
    "        nx.draw_networkx_labels(self.Graphs[which].G, pos=pos, labels = labels, font_size = 8, verticalalignment = 'top', horizontalalignment='right')\n",
    "        nx.draw_networkx_nodes(self.Graphs[which].G, pos=pos, nodelist = self.nodes_df, node_size = 40, node_color = 'red') \n",
    "        \n",
    "        return        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = ontologyGraphWithData(df_adata_comb_few)\n",
    "g.load_values()\n",
    "g.draw_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: fix layout with pos\n",
    "#TODO: add graph with altair\n",
    "#TODO: make graph interactively linked (hover, select) with heatmap / bars"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
